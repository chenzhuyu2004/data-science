{
 "cells": [
  {
   "cell_type": "code",
   "id": "11056403-bc9f-4e02-892d-963f8e922e35",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from keras_tuner import BayesianOptimization\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import shutil  # 导入 shutil 模块\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 读取数据\n",
    "X_train_scaled = pd.read_csv('X_train_scaled.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values[:, 0].squeeze()  # 选择第一列作为目标变量\n",
    "X_test_scaled = pd.read_csv('X_test_scaled.csv')\n",
    "y_test = pd.read_csv('y_test.csv').values[:, 0].squeeze()  # 同样选择第一列\n",
    "\n",
    "# 启用全局混合精度策略\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# 准备数据集的函数\n",
    "def create_dataset(X_data, y_data, batch_size=64):\n",
    "    return tf.data.Dataset.from_tensor_slices((X_data, y_data)).batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "# 创建训练集和验证集\n",
    "dataset_train = create_dataset(X_train_scaled, y_train)\n",
    "dataset_val = create_dataset(X_test_scaled, y_test)\n",
    "\n",
    "# 定义模型构建函数\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 15)):  # 最多15层\n",
    "        # L1 和 L2 正则化参数动态调整\n",
    "        l1_param = hp.Float(f'l1_{i}', min_value=0.0, max_value=1e-5, step=1e-6)\n",
    "        l2_param = hp.Float(f'l2_{i}', min_value=0.0, max_value=1e-5, step=1e-6)\n",
    "\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=1024, step=32),\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer=l1_l2(l1=l1_param, l2=l2_param)))\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.06, max_value=0.5, step=0.02)))\n",
    "\n",
    "    model.add(Dense(1, dtype='float32'))\n",
    "    optimizer = AdamW(learning_rate=hp.Float('learning_rate', min_value=1e-6, max_value=1e-2, sampling='log'), clipnorm=1.0)  # 添加梯度剪裁\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# 定义回调函数用于记录和保存损失\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, trial_dir):\n",
    "        super().__init__()\n",
    "        self.trial_dir = trial_dir\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    # 每个 epoch 结束后记录训练损失和验证损失\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1} - Loss: {logs.get('loss')}, Val Loss: {logs.get('val_loss')}\")\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.save_losses()\n",
    "\n",
    "    # 保存损失值为 CSV 文件\n",
    "    def save_losses(self):\n",
    "        losses_file = os.path.join(self.trial_dir, 'losses.csv')\n",
    "        val_losses_file = os.path.join(self.trial_dir, 'val_losses.csv')\n",
    "        np.savetxt(losses_file, self.losses, delimiter=\",\")\n",
    "        np.savetxt(val_losses_file, self.val_losses, delimiter=\",\")\n",
    "        print(f\"Saved losses to {losses_file} and {val_losses_file}\")\n",
    "\n",
    "# 自定义 tuner 类，用于传递 LossHistory 回调并根据试验结果调整超参数\n",
    "class AdaptiveTuner(BayesianOptimization):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.trial_results = []  # 存储每个试验的结果\n",
    "\n",
    "    # 在每个 trial 中运行模型\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        trial_id = trial.trial_id\n",
    "        trial_dir = os.path.join(self.directory, self.project_name, f'trial_{trial_id}')\n",
    "        os.makedirs(trial_dir, exist_ok=True)\n",
    "\n",
    "        # 使用 LossHistory 回调记录每个 trial 的损失\n",
    "        history = LossHistory(trial_dir)\n",
    "        callbacks = [history]\n",
    "\n",
    "        # 添加 ModelCheckpoint 回调，保存验证损失最小的模型\n",
    "        model_checkpoint = ModelCheckpoint(filepath=os.path.join(trial_dir, f'trial_{trial_id}_MLP_best_model.keras'),\n",
    "                                           monitor='val_loss', save_best_only=True)\n",
    "        callbacks.append(model_checkpoint)\n",
    "\n",
    "        # 传递回调\n",
    "        kwargs['callbacks'] = callbacks\n",
    "        result = super().run_trial(trial, *args, **kwargs)\n",
    "\n",
    "        # 记录 trial 的验证损失\n",
    "        if history.val_losses:\n",
    "            self.trial_results.append((trial, history.val_losses[-1]))\n",
    "        else:\n",
    "            self.trial_results.append((trial, None))  # 如果验证损失为空，记录为 None\n",
    "\n",
    "        return result\n",
    "\n",
    "# 回调函数\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# 超参数搜索\n",
    "tuner = AdaptiveTuner(\n",
    "    build_model,\n",
    "    objective='val_loss',  # 优化目标是验证损失\n",
    "    max_trials=30,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='optimized_single_output_regression',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# 搜索超参数\n",
    "tuner.search(dataset_train, epochs=100, validation_data=dataset_val, callbacks=callbacks)\n",
    "\n",
    "# 找到最佳 trial\n",
    "project_dir = os.path.join('my_dir', 'optimized_single_output_regression')\n",
    "\n",
    "trial_dirs = [d for d in os.listdir(project_dir) if d.startswith('trial_')]\n",
    "trial_dirs.sort(key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_trial_id = None\n",
    "\n",
    "# 遍历所有 trial 文件夹，找出验证损失最低的 trial\n",
    "for trial_dir in trial_dirs:\n",
    "    val_losses_file = os.path.join(project_dir, f'{trial_dir}/val_losses.csv')\n",
    "    if os.path.exists(val_losses_file):\n",
    "        val_losses = np.loadtxt(val_losses_file, delimiter=\",\")\n",
    "        if len(val_losses) > 0:\n",
    "            min_val_loss = np.min(val_losses)\n",
    "            if min_val_loss < best_val_loss:\n",
    "                best_val_loss = min_val_loss\n",
    "                best_trial_id = trial_dir.split('_')[1]\n",
    "\n",
    "# 检查找到的最佳 trial 是否存在对应的模型文件\n",
    "if best_trial_id is not None:\n",
    "    best_model_path = os.path.join(project_dir, f'trial_{best_trial_id}', f'trial_{best_trial_id}_MLP_best_model.keras')\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        # 保存最佳模型为 final_best_MLP_model.keras（使用复制而不是移动）\n",
    "        final_model_path = 'final_best_MLP_model.keras'\n",
    "        shutil.copy(best_model_path, final_model_path)  # 使用 shutil.copy 复制文件\n",
    "\n",
    "        # 加载最佳模型\n",
    "        best_model = keras.models.load_model(final_model_path)\n",
    "\n",
    "        # 计算 MAE 和 MSE\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f'MAE: {mae}, MSE: {mse}')\n",
    "\n",
    "        # 绘制 R² 图\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(y_test, y_pred, edgecolors=(0, 0, 0))\n",
    "        plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=4)\n",
    "        plt.title(f'R²: {r2:.3f}')\n",
    "        plt.xlabel('True Values')\n",
    "        plt.ylabel('Predictions')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"模型文件不存在: {best_model_path}\")\n",
    "else:\n",
    "    print(\"未找到最佳 trial 的模型文件。\")\n",
    "\n",
    "# 绘制每个 trial 的训练损失和验证损失曲线\n",
    "def plot_trials_loss_data(trial_dirs, project_dir):\n",
    "    num_trials = len(trial_dirs)\n",
    "    if num_trials == 0:\n",
    "        print(\"No trials to plot.\")\n",
    "        return\n",
    "\n",
    "    cols = 5\n",
    "    rows = math.ceil(num_trials / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "    axes = axes.flatten() if rows > 1 else [axes]\n",
    "\n",
    "    for trial_idx in range(num_trials):\n",
    "        if trial_idx < num_trials:\n",
    "            trial_dir = trial_dirs[trial_idx]\n",
    "            losses_file = os.path.join(project_dir, trial_dir, 'losses.csv')\n",
    "            val_losses_file = os.path.join(project_dir, trial_dir, 'val_losses.csv')\n",
    "            \n",
    "            if os.path.exists(losses_file) and os.path.exists(val_losses_file):\n",
    "                losses = np.loadtxt(losses_file, delimiter=\",\")\n",
    "                val_losses = np.loadtxt(val_losses_file, delimiter=\",\")\n",
    "                \n",
    "                axes[trial_idx].plot(losses, label='Training Loss', color='blue')\n",
    "                axes[trial_idx].plot(val_losses, label='Validation Loss', color='orange')\n",
    "                axes[trial_idx].set_title(trial_dir)\n",
    "                axes[trial_idx].set_xlabel('Epochs')\n",
    "                axes[trial_idx].set_ylabel('Loss')\n",
    "                axes[trial_idx].legend()\n",
    "                axes[trial_idx].grid(True)\n",
    "            else:\n",
    "                axes[trial_idx].set_title(trial_dir)\n",
    "                axes[trial_idx].text(0.5, 0.5, 'No data', horizontalalignment='center', verticalalignment='center')\n",
    "                axes[trial_idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 绘制所有 trial 的损失数据\n",
    "plot_trials_loss_data(trial_dirs, project_dir)"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 12s]\n",
      "val_loss: 96.81008911132812\n",
      "\n",
      "Best val_loss So Far: 96.81008911132812\n",
      "Total elapsed time: 00h 00m 12s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "9                 |11                |num_layers\n",
      "2e-06             |3e-06             |l1_0\n",
      "0                 |0                 |l2_0\n",
      "864               |544               |units_0\n",
      "0.24              |0.14              |dropout_0\n",
      "1.2109e-05        |4.1086e-06        |learning_rate\n",
      "4e-06             |0                 |l1_1\n",
      "6e-06             |0                 |l2_1\n",
      "384               |32                |units_1\n",
      "0.18              |0.06              |dropout_1\n",
      "6e-06             |0                 |l1_2\n",
      "6e-06             |0                 |l2_2\n",
      "320               |32                |units_2\n",
      "0.28              |0.06              |dropout_2\n",
      "1e-06             |0                 |l1_3\n",
      "0                 |0                 |l2_3\n",
      "544               |32                |units_3\n",
      "0.46              |0.06              |dropout_3\n",
      "0                 |0                 |l1_4\n",
      "1e-05             |0                 |l2_4\n",
      "768               |32                |units_4\n",
      "0.42              |0.06              |dropout_4\n",
      "5e-06             |0                 |l1_5\n",
      "8e-06             |0                 |l2_5\n",
      "1024              |32                |units_5\n",
      "0.44              |0.06              |dropout_5\n",
      "6e-06             |0                 |l1_6\n",
      "5e-06             |0                 |l2_6\n",
      "544               |32                |units_6\n",
      "0.46              |0.06              |dropout_6\n",
      "6e-06             |0                 |l1_7\n",
      "3e-06             |0                 |l2_7\n",
      "160               |32                |units_7\n",
      "0.22              |0.06              |dropout_7\n",
      "2e-06             |0                 |l1_8\n",
      "2e-06             |0                 |l2_8\n",
      "768               |32                |units_8\n",
      "0.22              |0.06              |dropout_8\n",
      "5e-06             |0                 |l1_9\n",
      "0                 |0                 |l2_9\n",
      "928               |32                |units_9\n",
      "0.26              |0.06              |dropout_9\n",
      "5e-06             |0                 |l1_10\n",
      "7e-06             |0                 |l2_10\n",
      "64                |32                |units_10\n",
      "0.46              |0.06              |dropout_10\n",
      "\n",
      "Epoch 1/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 87.0522Epoch 1 - Loss: 86.82585906982422, Val Loss: 97.88519287109375\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 88ms/step - loss: 87.0145 - val_loss: 97.8852\n",
      "Epoch 2/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 86.9407Epoch 2 - Loss: 86.66351318359375, Val Loss: 97.79474639892578\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 56ms/step - loss: 86.8945 - val_loss: 97.7947\n",
      "Epoch 3/100\n",
      "\u001B[1m4/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 86.8682Epoch 3 - Loss: 86.5154800415039, Val Loss: 97.69833374023438\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step - loss: 86.7507 - val_loss: 97.6983\n",
      "Epoch 4/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 86.5935Epoch 4 - Loss: 86.33667755126953, Val Loss: 97.59524536132812\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 86.5507 - val_loss: 97.5952\n",
      "Epoch 5/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 86.4438Epoch 5 - Loss: 86.16707611083984, Val Loss: 97.4836196899414\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step - loss: 86.3976 - val_loss: 97.4836\n",
      "Epoch 6/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 86.2097Epoch 6 - Loss: 85.97857666015625, Val Loss: 97.36111450195312\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 86.1712 - val_loss: 97.3611\n",
      "Epoch 7/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 86.0202Epoch 7 - Loss: 85.74761962890625, Val Loss: 97.22570037841797\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step - loss: 85.9748 - val_loss: 97.2257\n",
      "Epoch 8/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 85.9008Epoch 8 - Loss: 85.55682373046875, Val Loss: 97.07437133789062\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 70ms/step - loss: 85.8435 - val_loss: 97.0744\n",
      "Epoch 9/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 85.5244Epoch 9 - Loss: 85.25618743896484, Val Loss: 96.903076171875\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step - loss: 85.4797 - val_loss: 96.9031\n",
      "Epoch 10/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 85.2361Epoch 10 - Loss: 84.96647644042969, Val Loss: 96.70882415771484\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 85.1911 - val_loss: 96.7088\n",
      "Epoch 11/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 84.9121Epoch 11 - Loss: 84.66453552246094, Val Loss: 96.4861831665039\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 84.8709 - val_loss: 96.4862\n",
      "Epoch 12/100\n",
      "\u001B[1m4/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 84.6999Epoch 12 - Loss: 84.28700256347656, Val Loss: 96.23278045654297\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 60ms/step - loss: 84.5622 - val_loss: 96.2328\n",
      "Epoch 13/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 84.1996Epoch 13 - Loss: 83.89633178710938, Val Loss: 95.94257354736328\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step - loss: 84.1491 - val_loss: 95.9426\n",
      "Epoch 14/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 83.7779Epoch 14 - Loss: 83.47754669189453, Val Loss: 95.61084747314453\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 83.7278 - val_loss: 95.6108\n",
      "Epoch 15/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 83.4369Epoch 15 - Loss: 83.04579162597656, Val Loss: 95.232666015625\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 60ms/step - loss: 83.3717 - val_loss: 95.2327\n",
      "Epoch 16/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 82.9337Epoch 16 - Loss: 82.57159423828125, Val Loss: 94.8025894165039\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step - loss: 82.8733 - val_loss: 94.8026\n",
      "Epoch 17/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 82.3561Epoch 17 - Loss: 82.0035629272461, Val Loss: 94.31534576416016\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 57ms/step - loss: 82.2974 - val_loss: 94.3153\n",
      "Epoch 18/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 81.3648Epoch 18 - Loss: 81.16127014160156, Val Loss: 93.76268005371094\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 81.3309 - val_loss: 93.7627\n",
      "Epoch 19/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 80.7731Epoch 19 - Loss: 80.55442810058594, Val Loss: 93.13851165771484\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 80.7366 - val_loss: 93.1385\n",
      "Epoch 20/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 80.0278Epoch 20 - Loss: 79.59458923339844, Val Loss: 92.43453216552734\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 79.9556 - val_loss: 92.4345\n",
      "Epoch 21/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 79.2224Epoch 21 - Loss: 78.727294921875, Val Loss: 91.6428451538086\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 72ms/step - loss: 79.1398 - val_loss: 91.6428\n",
      "Epoch 22/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 78.2021Epoch 22 - Loss: 77.81965637207031, Val Loss: 90.75411987304688\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 78.1384 - val_loss: 90.7541\n",
      "Epoch 23/100\n",
      "\u001B[1m4/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - loss: 77.2964Epoch 23 - Loss: 76.78570556640625, Val Loss: 89.75814056396484\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 77.1261 - val_loss: 89.7581\n",
      "Epoch 24/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 76.2611Epoch 24 - Loss: 75.8681411743164, Val Loss: 88.64887237548828\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 73ms/step - loss: 76.1956 - val_loss: 88.6489\n",
      "Epoch 25/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 74.8421Epoch 25 - Loss: 74.48147583007812, Val Loss: 87.42266845703125\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 74.7820 - val_loss: 87.4227\n",
      "Epoch 26/100\n",
      "\u001B[1m4/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 73.2582Epoch 26 - Loss: 72.65728759765625, Val Loss: 86.06883239746094\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step - loss: 73.0579 - val_loss: 86.0688\n",
      "Epoch 27/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 71.8043Epoch 27 - Loss: 71.24921417236328, Val Loss: 84.57486724853516\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 71.7118 - val_loss: 84.5749\n",
      "Epoch 28/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 71.0496Epoch 28 - Loss: 70.49229431152344, Val Loss: 82.94114685058594\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 64ms/step - loss: 70.9567 - val_loss: 82.9411\n",
      "Epoch 29/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 69.2669Epoch 29 - Loss: 68.43829345703125, Val Loss: 81.16648864746094\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step - loss: 69.1288 - val_loss: 81.1665\n",
      "Epoch 30/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 67.5202Epoch 30 - Loss: 67.1266098022461, Val Loss: 79.24545288085938\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 67.4546 - val_loss: 79.2455\n",
      "Epoch 31/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 65.8879Epoch 31 - Loss: 65.17002868652344, Val Loss: 77.18154907226562\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 72ms/step - loss: 65.7683 - val_loss: 77.1815\n",
      "Epoch 32/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 63.9290Epoch 32 - Loss: 63.3113899230957, Val Loss: 74.98908996582031\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step - loss: 63.8261 - val_loss: 74.9891\n",
      "Epoch 33/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 62.6833Epoch 33 - Loss: 61.73246383666992, Val Loss: 72.6869125366211\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 62.5249 - val_loss: 72.6869\n",
      "Epoch 34/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 59.7663Epoch 34 - Loss: 59.53983688354492, Val Loss: 70.27336883544922\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 71ms/step - loss: 59.7286 - val_loss: 70.2734\n",
      "Epoch 35/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 58.2495Epoch 35 - Loss: 57.432395935058594, Val Loss: 67.76514434814453\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 74ms/step - loss: 58.1133 - val_loss: 67.7651\n",
      "Epoch 36/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 56.7582Epoch 36 - Loss: 56.26718521118164, Val Loss: 65.20262145996094\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 77ms/step - loss: 56.6763 - val_loss: 65.2026\n",
      "Epoch 37/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 54.3152Epoch 37 - Loss: 53.842491149902344, Val Loss: 62.590728759765625\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 72ms/step - loss: 54.2364 - val_loss: 62.5907\n",
      "Epoch 38/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 53.0094Epoch 38 - Loss: 52.478336334228516, Val Loss: 60.010005950927734\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 72ms/step - loss: 52.9209 - val_loss: 60.0100\n",
      "Epoch 39/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 52.6724Epoch 39 - Loss: 52.04684829711914, Val Loss: 57.531002044677734\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 52.5681 - val_loss: 57.5310\n",
      "Epoch 40/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 49.7621Epoch 40 - Loss: 49.8487663269043, Val Loss: 55.24245834350586\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 49.7765 - val_loss: 55.2425\n",
      "Epoch 41/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 49.1191Epoch 41 - Loss: 48.611061096191406, Val Loss: 53.218482971191406\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 70ms/step - loss: 49.0345 - val_loss: 53.2185\n",
      "Epoch 42/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 47.9187Epoch 42 - Loss: 47.30179214477539, Val Loss: 51.3345947265625\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 60ms/step - loss: 47.8159 - val_loss: 51.3346\n",
      "Epoch 43/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 45.5883Epoch 43 - Loss: 46.296539306640625, Val Loss: 49.530174255371094\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 45.7063 - val_loss: 49.5302\n",
      "Epoch 44/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 46.1965Epoch 44 - Loss: 45.68490982055664, Val Loss: 47.89509201049805\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step - loss: 46.1112 - val_loss: 47.8951\n",
      "Epoch 45/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 45.1138Epoch 45 - Loss: 44.6589469909668, Val Loss: 46.33186721801758\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step - loss: 45.0380 - val_loss: 46.3319\n",
      "Epoch 46/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 43.6688Epoch 46 - Loss: 42.536434173583984, Val Loss: 44.9226188659668\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 43.4801 - val_loss: 44.9226\n",
      "Epoch 47/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 41.6279Epoch 47 - Loss: 41.42729187011719, Val Loss: 43.5268669128418\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step - loss: 41.5945 - val_loss: 43.5269\n",
      "Epoch 48/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 39.9923Epoch 48 - Loss: 40.63562774658203, Val Loss: 42.21959686279297\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 40.0996 - val_loss: 42.2196\n",
      "Epoch 49/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 41.3320Epoch 49 - Loss: 40.88644790649414, Val Loss: 41.07133102416992\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step - loss: 41.2578 - val_loss: 41.0713\n",
      "Epoch 50/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 41.3986Epoch 50 - Loss: 40.418060302734375, Val Loss: 40.120635986328125\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 70ms/step - loss: 41.2352 - val_loss: 40.1206\n",
      "Epoch 51/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 38.0047Epoch 51 - Loss: 38.18620300292969, Val Loss: 39.092559814453125\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 73ms/step - loss: 38.0350 - val_loss: 39.0926\n",
      "Epoch 52/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 38.0732Epoch 52 - Loss: 37.2427978515625, Val Loss: 38.06270980834961\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step - loss: 37.9348 - val_loss: 38.0627\n",
      "Epoch 53/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 37.8556Epoch 53 - Loss: 37.51118850708008, Val Loss: 37.08964157104492\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step - loss: 37.7982 - val_loss: 37.0896\n",
      "Epoch 54/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 36.7810Epoch 54 - Loss: 36.179161071777344, Val Loss: 36.23799133300781\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 36.6807 - val_loss: 36.2380\n",
      "Epoch 55/100\n",
      "\u001B[1m4/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 37.2091Epoch 55 - Loss: 35.48969650268555, Val Loss: 35.455875396728516\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step - loss: 36.6360 - val_loss: 35.4559\n",
      "Epoch 56/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 34.8423Epoch 56 - Loss: 34.901920318603516, Val Loss: 34.721839904785156\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 70ms/step - loss: 34.8522 - val_loss: 34.7218\n",
      "Epoch 57/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 33.7695Epoch 57 - Loss: 33.78383255004883, Val Loss: 34.08154296875\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step - loss: 33.7719 - val_loss: 34.0815\n",
      "Epoch 58/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 34.8207Epoch 58 - Loss: 33.37591552734375, Val Loss: 33.55303192138672\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step - loss: 34.5799 - val_loss: 33.5530\n",
      "Epoch 59/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 35.3803Epoch 59 - Loss: 35.01255798339844, Val Loss: 33.06088638305664\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 73ms/step - loss: 35.3190 - val_loss: 33.0609\n",
      "Epoch 60/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 33.5118Epoch 60 - Loss: 33.692752838134766, Val Loss: 32.54775619506836\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 33.5420 - val_loss: 32.5478\n",
      "Epoch 61/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 32.9288Epoch 61 - Loss: 32.20067596435547, Val Loss: 32.04051971435547\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 32.8074 - val_loss: 32.0405\n",
      "Epoch 62/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 33.6615Epoch 62 - Loss: 31.86284828186035, Val Loss: 31.46358299255371\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 70ms/step - loss: 33.3617 - val_loss: 31.4636\n",
      "Epoch 63/100\n",
      "\u001B[1m4/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 29.7046Epoch 63 - Loss: 29.406400680541992, Val Loss: 30.7265625\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step - loss: 29.6052 - val_loss: 30.7266\n",
      "Epoch 64/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 28.6368Epoch 64 - Loss: 29.454357147216797, Val Loss: 29.954608917236328\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 57ms/step - loss: 28.7731 - val_loss: 29.9546\n",
      "Epoch 65/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 28.3476Epoch 65 - Loss: 28.12750244140625, Val Loss: 29.363643646240234\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 28.3109 - val_loss: 29.3636\n",
      "Epoch 66/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 29.5111Epoch 66 - Loss: 29.210548400878906, Val Loss: 28.90375518798828\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 60ms/step - loss: 29.4610 - val_loss: 28.9038\n",
      "Epoch 67/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 29.7243Epoch 67 - Loss: 29.354787826538086, Val Loss: 28.4394474029541\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step - loss: 29.6627 - val_loss: 28.4394\n",
      "Epoch 68/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 28.3170Epoch 68 - Loss: 27.68500328063965, Val Loss: 28.065929412841797\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 28.2117 - val_loss: 28.0659\n",
      "Epoch 69/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 27.6897Epoch 69 - Loss: 27.509462356567383, Val Loss: 27.73180389404297\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 27.6597 - val_loss: 27.7318\n",
      "Epoch 70/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 29.4941Epoch 70 - Loss: 29.195802688598633, Val Loss: 27.451332092285156\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 71ms/step - loss: 29.4444 - val_loss: 27.4513\n",
      "Epoch 71/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 27.4101Epoch 71 - Loss: 27.942413330078125, Val Loss: 27.264577865600586\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 72ms/step - loss: 27.4988 - val_loss: 27.2646\n",
      "Epoch 72/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 26.8824Epoch 72 - Loss: 27.559402465820312, Val Loss: 27.081806182861328\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 56ms/step - loss: 26.9952 - val_loss: 27.0818\n",
      "Epoch 73/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 28.9667Epoch 73 - Loss: 27.66804313659668, Val Loss: 26.784223556518555\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 70ms/step - loss: 28.7502 - val_loss: 26.7842\n",
      "Epoch 74/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 26.6962Epoch 74 - Loss: 26.256685256958008, Val Loss: 26.443111419677734\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step - loss: 26.6230 - val_loss: 26.4431\n",
      "Epoch 75/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 25.3857Epoch 75 - Loss: 25.230464935302734, Val Loss: 26.159807205200195\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step - loss: 25.3599 - val_loss: 26.1598\n",
      "Epoch 76/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 24.9279Epoch 76 - Loss: 24.4870548248291, Val Loss: 25.724990844726562\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 71ms/step - loss: 24.8545 - val_loss: 25.7250\n",
      "Epoch 77/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 27.2109Epoch 77 - Loss: 26.454166412353516, Val Loss: 25.495594024658203\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 71ms/step - loss: 27.0848 - val_loss: 25.4956\n",
      "Epoch 78/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 25.1281Epoch 78 - Loss: 24.094594955444336, Val Loss: 25.215078353881836\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step - loss: 24.9558 - val_loss: 25.2151\n",
      "Epoch 79/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 25.2174Epoch 79 - Loss: 25.104461669921875, Val Loss: 24.943519592285156\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 73ms/step - loss: 25.1986 - val_loss: 24.9435\n",
      "Epoch 80/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 25.7128Epoch 80 - Loss: 25.48957061767578, Val Loss: 24.70912742614746\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 25.6756 - val_loss: 24.7091\n",
      "Epoch 81/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 24.6703Epoch 81 - Loss: 24.32122039794922, Val Loss: 24.422657012939453\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step - loss: 24.6121 - val_loss: 24.4227\n",
      "Epoch 82/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 23.8400Epoch 82 - Loss: 23.44370460510254, Val Loss: 24.13173484802246\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step - loss: 23.7739 - val_loss: 24.1317\n",
      "Epoch 83/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 23.0280Epoch 83 - Loss: 22.791168212890625, Val Loss: 23.834558486938477\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step - loss: 22.9885 - val_loss: 23.8346\n",
      "Epoch 84/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 23.4048Epoch 84 - Loss: 23.30152130126953, Val Loss: 23.585647583007812\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - loss: 23.3875 - val_loss: 23.5856\n",
      "Epoch 85/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 22.9668Epoch 85 - Loss: 22.347557067871094, Val Loss: 23.280231475830078\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step - loss: 22.8636 - val_loss: 23.2802\n",
      "Epoch 86/100\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 22.2535Epoch 86 - Loss: 22.050939559936523, Val Loss: 22.99851417541504\n",
      "Saved losses to my_dir\\optimized_single_output_regression\\trial_01\\losses.csv and my_dir\\optimized_single_output_regression\\trial_01\\val_losses.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 131\u001B[0m\n\u001B[0;32m    120\u001B[0m tuner \u001B[38;5;241m=\u001B[39m AdaptiveTuner(\n\u001B[0;32m    121\u001B[0m     build_model,\n\u001B[0;32m    122\u001B[0m     objective\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# 优化目标是验证损失\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    127\u001B[0m     overwrite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    128\u001B[0m )\n\u001B[0;32m    130\u001B[0m \u001B[38;5;66;03m# 搜索超参数\u001B[39;00m\n\u001B[1;32m--> 131\u001B[0m tuner\u001B[38;5;241m.\u001B[39msearch(dataset_train, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, validation_data\u001B[38;5;241m=\u001B[39mdataset_val, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n\u001B[0;32m    133\u001B[0m \u001B[38;5;66;03m# 找到最佳 trial\u001B[39;00m\n\u001B[0;32m    134\u001B[0m project_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmy_dir\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimized_single_output_regression\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001B[0m, in \u001B[0;36mBaseTuner.search\u001B[1;34m(self, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    231\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_trial_begin(trial)\n\u001B[1;32m--> 234\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_run_and_update_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_trial_end(trial)\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_search_end()\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001B[0m, in \u001B[0;36mBaseTuner._try_run_and_update_trial\u001B[1;34m(self, trial, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_run_and_update_trial\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs):\n\u001B[0;32m    273\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 274\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_and_update_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    275\u001B[0m         trial\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m trial_module\u001B[38;5;241m.\u001B[39mTrialStatus\u001B[38;5;241m.\u001B[39mCOMPLETED\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001B[0m, in \u001B[0;36mBaseTuner._run_and_update_trial\u001B[1;34m(self, trial, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_and_update_trial\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs):\n\u001B[1;32m--> 239\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mget_trial(trial\u001B[38;5;241m.\u001B[39mtrial_id)\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mexists(\n\u001B[0;32m    241\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mobjective\u001B[38;5;241m.\u001B[39mname\n\u001B[0;32m    242\u001B[0m     ):\n\u001B[0;32m    243\u001B[0m         \u001B[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001B[39;00m\n\u001B[0;32m    244\u001B[0m         \u001B[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001B[39;00m\n\u001B[0;32m    245\u001B[0m         \u001B[38;5;66;03m# use case. No further action needed in this case.\u001B[39;00m\n\u001B[0;32m    246\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    247\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe use case of calling \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    248\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    254\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    255\u001B[0m         )\n",
      "Cell \u001B[1;32mIn[1], line 103\u001B[0m, in \u001B[0;36mAdaptiveTuner.run_trial\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# 传递回调\u001B[39;00m\n\u001B[0;32m    102\u001B[0m kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m callbacks\n\u001B[1;32m--> 103\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrun_trial(trial, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    105\u001B[0m \u001B[38;5;66;03m# 记录 trial 的验证损失\u001B[39;00m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m history\u001B[38;5;241m.\u001B[39mval_losses:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001B[0m, in \u001B[0;36mTuner.run_trial\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    312\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(model_checkpoint)\n\u001B[0;32m    313\u001B[0m     copied_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m callbacks\n\u001B[1;32m--> 314\u001B[0m     obj_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_and_fit_model(trial, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcopied_kwargs)\n\u001B[0;32m    316\u001B[0m     histories\u001B[38;5;241m.\u001B[39mappend(obj_value)\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m histories\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001B[0m, in \u001B[0;36mTuner._build_and_fit_model\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    231\u001B[0m hp \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39mhyperparameters\n\u001B[0;32m    232\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_build(hp)\n\u001B[1;32m--> 233\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhypermodel\u001B[38;5;241m.\u001B[39mfit(hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    235\u001B[0m \u001B[38;5;66;03m# Save the build config for model loading later.\u001B[39;00m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m backend\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmulti_backend():\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001B[0m, in \u001B[0;36mHyperModel.fit\u001B[1;34m(self, hp, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    126\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Train the model.\u001B[39;00m\n\u001B[0;32m    127\u001B[0m \n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;124;03m        If return a float, it should be the `objective` value.\u001B[39;00m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mfit(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:359\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[0;32m    354\u001B[0m     val_logs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    355\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    356\u001B[0m     }\n\u001B[0;32m    357\u001B[0m     epoch_logs\u001B[38;5;241m.\u001B[39mupdate(val_logs)\n\u001B[1;32m--> 359\u001B[0m callbacks\u001B[38;5;241m.\u001B[39mon_epoch_end(epoch, epoch_logs)\n\u001B[0;32m    360\u001B[0m training_logs \u001B[38;5;241m=\u001B[39m epoch_logs\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:151\u001B[0m, in \u001B[0;36mCallbackList.on_epoch_end\u001B[1;34m(self, epoch, logs)\u001B[0m\n\u001B[0;32m    149\u001B[0m logs \u001B[38;5;241m=\u001B[39m logs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[1;32m--> 151\u001B[0m     callback\u001B[38;5;241m.\u001B[39mon_epoch_end(epoch, logs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206\u001B[0m, in \u001B[0;36mModelCheckpoint.on_epoch_end\u001B[1;34m(self, epoch, logs)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_epoch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, epoch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_freq \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 206\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_model(epoch\u001B[38;5;241m=\u001B[39mepoch, batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, logs\u001B[38;5;241m=\u001B[39mlogs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:273\u001B[0m, in \u001B[0;36mModelCheckpoint._save_model\u001B[1;34m(self, epoch, batch, logs)\u001B[0m\n\u001B[0;32m    271\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39msave_weights(filepath, overwrite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    272\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 273\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39msave(filepath, overwrite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    275\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\models\\model.py:309\u001B[0m, in \u001B[0;36mModel.save\u001B[1;34m(self, filepath, overwrite, zipped, **kwargs)\u001B[0m\n\u001B[0;32m    267\u001B[0m \u001B[38;5;129m@traceback_utils\u001B[39m\u001B[38;5;241m.\u001B[39mfilter_traceback\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave\u001B[39m(\u001B[38;5;28mself\u001B[39m, filepath, overwrite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, zipped\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    269\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Saves a model as a `.keras` file.\u001B[39;00m\n\u001B[0;32m    270\u001B[0m \n\u001B[0;32m    271\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;124;03m    Thus models can be reinstantiated in the exact same state.\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saving_api\u001B[38;5;241m.\u001B[39msave_model(\n\u001B[0;32m    310\u001B[0m         \u001B[38;5;28mself\u001B[39m, filepath, overwrite\u001B[38;5;241m=\u001B[39moverwrite, zipped\u001B[38;5;241m=\u001B[39mzipped, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    311\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:107\u001B[0m, in \u001B[0;36msave_model\u001B[1;34m(model, filepath, overwrite, zipped, **kwargs)\u001B[0m\n\u001B[0;32m    104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m zipped \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(filepath)\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saving_lib\u001B[38;5;241m.\u001B[39msave_model(model, filepath)\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m zipped:\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saving_lib\u001B[38;5;241m.\u001B[39msave_model(model, filepath, zipped\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:142\u001B[0m, in \u001B[0;36msave_model\u001B[1;34m(model, filepath, weights_format, zipped)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filepath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m--> 142\u001B[0m         _save_model_to_fileobj(model, f, weights_format)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:245\u001B[0m, in \u001B[0;36m_save_model_to_fileobj\u001B[1;34m(model, fileobj, weights_format)\u001B[0m\n\u001B[0;32m    237\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    238\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown `weights_format` argument. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    239\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh5\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnpz\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    240\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived: weights_format=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mweights_format\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    241\u001B[0m         )\n\u001B[0;32m    243\u001B[0m     asset_store \u001B[38;5;241m=\u001B[39m DiskIOStore(_ASSETS_DIRNAME, archive\u001B[38;5;241m=\u001B[39mzf, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 245\u001B[0m     _save_state(\n\u001B[0;32m    246\u001B[0m         model,\n\u001B[0;32m    247\u001B[0m         weights_store\u001B[38;5;241m=\u001B[39mweights_store,\n\u001B[0;32m    248\u001B[0m         assets_store\u001B[38;5;241m=\u001B[39masset_store,\n\u001B[0;32m    249\u001B[0m         inner_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    250\u001B[0m         visited_saveables\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mset\u001B[39m(),\n\u001B[0;32m    251\u001B[0m     )\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m     \u001B[38;5;66;03m# Skip the final `zf.write` if any exception is raised\u001B[39;00m\n\u001B[0;32m    254\u001B[0m     write_zf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:688\u001B[0m, in \u001B[0;36m_save_state\u001B[1;34m(saveable, weights_store, assets_store, inner_path, visited_saveables)\u001B[0m\n\u001B[0;32m    678\u001B[0m     _save_state(\n\u001B[0;32m    679\u001B[0m         child_obj,\n\u001B[0;32m    680\u001B[0m         weights_store,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    685\u001B[0m         visited_saveables\u001B[38;5;241m=\u001B[39mvisited_saveables,\n\u001B[0;32m    686\u001B[0m     )\n\u001B[0;32m    687\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(child_obj, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mdict\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mset\u001B[39m)):\n\u001B[1;32m--> 688\u001B[0m     _save_container_state(\n\u001B[0;32m    689\u001B[0m         child_obj,\n\u001B[0;32m    690\u001B[0m         weights_store,\n\u001B[0;32m    691\u001B[0m         assets_store,\n\u001B[0;32m    692\u001B[0m         inner_path\u001B[38;5;241m=\u001B[39mfile_utils\u001B[38;5;241m.\u001B[39mjoin(inner_path, child_attr)\u001B[38;5;241m.\u001B[39mreplace(\n\u001B[0;32m    693\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    694\u001B[0m         ),\n\u001B[0;32m    695\u001B[0m         visited_saveables\u001B[38;5;241m=\u001B[39mvisited_saveables,\n\u001B[0;32m    696\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:805\u001B[0m, in \u001B[0;36m_save_container_state\u001B[1;34m(container, weights_store, assets_store, inner_path, visited_saveables)\u001B[0m\n\u001B[0;32m    803\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    804\u001B[0m     used_names[name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 805\u001B[0m _save_state(\n\u001B[0;32m    806\u001B[0m     saveable,\n\u001B[0;32m    807\u001B[0m     weights_store,\n\u001B[0;32m    808\u001B[0m     assets_store,\n\u001B[0;32m    809\u001B[0m     inner_path\u001B[38;5;241m=\u001B[39mfile_utils\u001B[38;5;241m.\u001B[39mjoin(inner_path, name)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    810\u001B[0m     visited_saveables\u001B[38;5;241m=\u001B[39mvisited_saveables,\n\u001B[0;32m    811\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:667\u001B[0m, in \u001B[0;36m_save_state\u001B[1;34m(saveable, weights_store, assets_store, inner_path, visited_saveables)\u001B[0m\n\u001B[0;32m    665\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    666\u001B[0m         metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 667\u001B[0m     saveable\u001B[38;5;241m.\u001B[39msave_own_variables(\n\u001B[0;32m    668\u001B[0m         weights_store\u001B[38;5;241m.\u001B[39mmake(inner_path, metadata\u001B[38;5;241m=\u001B[39mmetadata)\n\u001B[0;32m    669\u001B[0m     )\n\u001B[0;32m    670\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(saveable, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_assets\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m assets_store:\n\u001B[0;32m    671\u001B[0m     saveable\u001B[38;5;241m.\u001B[39msave_assets(assets_store\u001B[38;5;241m.\u001B[39mmake(inner_path))\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:215\u001B[0m, in \u001B[0;36mDense.save_own_variables\u001B[1;34m(self, store)\u001B[0m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_quantization_mode_error(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquantization_mode)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, variable \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(target_variables):\n\u001B[1;32m--> 215\u001B[0m     store[\u001B[38;5;28mstr\u001B[39m(i)] \u001B[38;5;241m=\u001B[39m variable\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "id": "2f935c5a-7078-465e-93d3-fb6ae9c4c107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T05:52:29.049620Z",
     "start_time": "2024-10-12T05:52:28.922651Z"
    }
   },
   "source": [
    "# 合并训练集和测试集\n",
    "X_combined = np.vstack((X_train_scaled, X_test_scaled))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "# 使用最佳模型进行预测\n",
    "y_pred_combined = best_model.predict(X_combined)\n",
    "\n",
    "# 计算合并数据集上的 R² 值\n",
    "r2_combined = r2_score(y_combined, y_pred_combined)\n",
    "\n",
    "# 绘制 R² 图\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_combined, y_pred_combined, edgecolors=(0, 0, 0))\n",
    "plt.plot([min(y_combined), max(y_combined)], [min(y_combined), max(y_combined)], 'k--', lw=4)\n",
    "plt.title(f'R² (combined): {r2_combined:.3f}')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Combined R²: {r2_combined}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m13/13\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined R²: 0.9043537012274686\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cf8fb5daf78ad5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
